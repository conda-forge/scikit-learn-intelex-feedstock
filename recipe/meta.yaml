# NB: this recipe should be synced between sklearnex repo and feedstocks

{% set name = "scikit-learn-intelex" %}
{% set buildnumber = 0 %}
# version is set manually in feedstocks and through git tag in repo
{% set version = "2025.10.1" %}

package:
  name: {{ name|lower }}
  # version: {{ environ.get('GIT_DESCRIBE_TAG') }}
  version: {{ version }}

# Reminder: source should point to path for repo or github archive for feedstock
source:
  url: https://github.com/uxlfoundation/scikit-learn-intelex/archive/{{ version }}.tar.gz
  sha256: 25a4dc3f83fe48d88a59133e57007584b3115cff97a2caab7e8be3c5c608763c
  patches:
  - patches/0004-accidental-bad-tests.patch
  - patches/0005-skip-memleak-random-fails.patch
# source:
#   path: ..

build:
  skip: true  # [not (linux64 or win)]
  number: {{ buildnumber }}
  include_recipe: false
  script_env:
    - DPCPPROOT
    - DALROOT
    - NO_DIST=1  # [win]
  ignore_run_exports_from:
    - {{ compiler('dpcpp') }}  # [linux64]

requirements:
  build:
    - make  # [linux]
    - {{ compiler('dpcpp') }}  # [linux64]
    - {{ compiler('cxx') }}
    # conda-forge feedstock specific
    - {{ stdlib("c") }}
  host:
    - python
    - setuptools
    - cmake
    - clang-format
    - cython
    - jinja2
    - pybind11
    - numpy
    # dal-devel pinning depends on the recipe location (repo or feedstock)
    # - dal-devel
    - dal-devel ==2025.10.0
    - impi-devel  # [not win]
    - mpi * impi  # [not win]
  run:
    - python
    - numpy
    - scikit-learn
    - scikit-learn  <1.6  # [py<=39]
    # dal-devel has run_exports on conda-forge, so no need to add it.
    - mpi * impi  # [not win]
    - packaging  # [py>=313]

test:
  requires:
    - pyyaml
    - mpi * impi  # [not win]
    - mpi4py
    # DPC part of sklearnex is optional
    - dpcpp-cpp-rt  # [linux64]
    # TODO: enable data parallel frameworks when they are available on conda-forge
    # - dpctl
    # - dpnp
    # next deps are synced with requirements-test.txt
    - pytest
    - pytest-mock
    - pandas
    - xgboost
    - lightgbm
    - treelite
    # TODO: re-enable shap once they have py314 support and once they sort
    # out their issues with numba version pinnings
    # - shap
    # TODO: re-enable once their py314 builds work correctly
    - catboost  # [py<=313]
    - array-api-compat
    - array-api-strict
    - array-api-strict <2.4  # [py<312]
  source_files:
    - .ci
    - setup.cfg
    - examples
    - tests

about:
  home: https://uxlfoundation.github.io/scikit-learn-intelex
  license: Apache-2.0
  license_file:
    - LICENSE
    - doc/third-party-programs-sklearnex.txt
  summary: Extension for Scikit-learn* is a seamless way to speed up your Scikit-learn application.
  description: |
    Extension for Scikit-learn is a <strong>free software AI accelerator</strong> designed to deliver over
    <strong>10-100X</strong> acceleration to your existing scikit-learn code. The software acceleration is
    achieved with vector instructions, AI hardware-specific memory optimizations, threading, and optimizations.
    <br/><br/>
    With Extension for Scikit-learn, you can:
    <ul>
    <li>Speed up training and inference by up to 100x with equivalent mathematical accuracy</li>
    <li>Benefit from performance improvements across different hardware configurations, including
        <a href="https://uxlfoundation.github.io/scikit-learn-intelex/latest/oneapi-gpu.html" target="_blank">GPUs</a>
        and <a href="https://uxlfoundation.github.io/scikit-learn-intelex/latest/distributed-mode.html" target="_blank">multi-GPU</a>
        configurations
    </li>
    <li>Integrate the extension into your existing Scikit-learn applications without code modifications</li>
    <li>Continue to use the open-source scikit-learn API</li>
    <li>Enable and disable the extension with a couple of lines of code or at the command line</li>
    </ul>
  dev_url: https://github.com/uxlfoundation/scikit-learn-intelex
  doc_url: https://uxlfoundation.github.io/scikit-learn-intelex

extra:
  recipe-maintainers:
    # GitHub IDs for maintainers of the recipe.
    - napetrov
    - david-cortes-intel
    - maria-Petrova
    - ethanglaser
